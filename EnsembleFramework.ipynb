{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N79oufXtyDJC"
      },
      "source": [
        "# Workspace setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "colab_type": "code",
        "id": "yEfJd4dFt7o7",
        "outputId": "549d6eb5-ebc7-4b45-fd55-3028daa0f6ef"
      },
      "outputs": [],
      "source": [
        "# !pip install pyxdameraulevenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NhZGrQfL4JNK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import tree\n",
        "from scipy.stats import percentileofscore\n",
        "from scipy.stats import iqr\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import entropy\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import kurtosis\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4ftTeWKs4JNO"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "colab_type": "code",
        "id": "6qvfYYA14kXu",
        "outputId": "04c3e21c-d7cc-4af3-ecd1-62bef3f8fbb3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount = True)\n",
        "WORK_DIR = '/home/muhammadinan/Documents/GitHub/IoT_Classification/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4eOwAiFz4JNQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(f'{WORK_DIR}')\n",
        "\n",
        "from BOS_Wrapper import BOS_Classifier\n",
        "from NLP_Classifier import NLP_Classifier\n",
        "from TSC_1NN import TSC_1NN, DTWDistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RqlxloxZ4JNV"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOVA0xdT4JNX"
      },
      "source": [
        "# EnsembleFramework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lwGqvxs-4JNX"
      },
      "outputs": [],
      "source": [
        "class EnsembleFramework:\n",
        "    \n",
        "    def __init__(self, criterion = 'topk', tuning = False, \n",
        "                 layers = [{'type' : 'NLP'}, {'type' : 'BOS', 'name' : 'DecisionTreeClassifier()'}], \n",
        "                 params = {'k' : [4, 1]}):\n",
        "        self.criterion = criterion\n",
        "        self.layers = layers\n",
        "        self.params = params\n",
        "        self.tuning = tuning\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\"criterion\": self.criterion, \n",
        "                \"layers\": self.layers,\n",
        "                \"params\": self.params,\n",
        "                \"tuning\": self.tuning}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "    \n",
        "    # SoF filtering - receives in input the list of the class probabilities and the list of the remaining classes\n",
        "    def survival(self, probs, classes):\n",
        "        survived = []\n",
        "  \n",
        "        # selecting the probabilities of the remaining classes\n",
        "        survived_probs = [probs[c] for c in classes]\n",
        "\n",
        "        # mean and std\n",
        "        media = np.mean(survived_probs)\n",
        "        std_dev = np.std(survived_probs)\n",
        "        \n",
        "        # filtering based on sigma value\n",
        "        for c in classes:\n",
        "            if (probs[c] >= (media + self.params['sigma']*std_dev)):\n",
        "                survived.append(c)\n",
        "        \n",
        "        # if the final list is empty, we select the most probable class\n",
        "        if (len(survived) == 0):\n",
        "            survived = nlargest(1, classes, key = lambda x : probs[x])\n",
        "            \n",
        "        return survived\n",
        "    \n",
        "\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        # class list\n",
        "        self.classes = set(y_train)\n",
        "        \n",
        "        # classifier pipeline\n",
        "        self.classifiers = []\n",
        "        input_clf = []\n",
        "        \n",
        "        #X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train)\n",
        "        \n",
        "        # let's populate our pipeline\n",
        "        for classifier in self.layers:\n",
        "            if (classifier['type'] == 'BOS'):\n",
        "                clf = BOS_Classifier(eval(classifier['name']))\n",
        "            elif (classifier['type'] == 'NLP'):\n",
        "                clf = NLP_Classifier()\n",
        "            elif (classifier['type'] == 'TSC'):\n",
        "                clf = eval(classifier['name'])\n",
        "            clf.fit(X_train, y_train)\n",
        "            #input_clf.append(clf)\n",
        "            self.classifiers.append(clf)\n",
        "          \n",
        "        #self.classifiers = self.construct(input_clf, X_val, y_val)\n",
        " \n",
        "    # populating our pipeline according to the maximum mean variance method\n",
        "    # NOT USED IN THE FINAL VERSION\n",
        "    def construct(self, candidates, X_val, y_val, max_iterations = 5):\n",
        "\n",
        "        val_classes = [list(set(y_val))] * len(X_val)\n",
        "        n_iter = 0\n",
        "        selected = []\n",
        "        probs = []\n",
        "\n",
        "        for clf in candidates:\n",
        "            probs.append(clf.predict_proba(X_val)) \n",
        "\n",
        "        if (self.criterion == 'sof'):\n",
        "            while(any(x > 1 for x in [len(x) for x in val_classes]) and n_iter < max_iterations):\n",
        "                n_iter = n_iter + 1\n",
        "                print(\"iteration \", n_iter)\n",
        "                max_variance, index = 0, 0\n",
        "                for j, _ in enumerate(candidates):\n",
        "                    variances = []\n",
        "                    for i, series_probs in enumerate(probs[j]):\n",
        "                        variances.append(np.var([series_probs[m] for m in val_classes[i]])) \n",
        "\n",
        "                    print(\"classifier number \", j, \" has variance \", iqr(variances))\n",
        "                    if iqr(variances) > max_variance :\n",
        "                        max_variance = iqr(variances)\n",
        "                        index = j\n",
        "                print(\"the best classifier is the \", index)\n",
        "                selected.append(candidates[index])\n",
        "\n",
        "                for i, series_probs in enumerate(probs[index]):\n",
        "                    val_classes[i] = self.survival(series_probs, val_classes[i])\n",
        "\n",
        "        return selected      \n",
        "\n",
        "    # dynamic prediction based on classifier confidence\n",
        "    # NOT USED IN THE FINAL VERSION\n",
        "    def predict_dinamic(self, X_test, max_iterations = 5, debug = False, y_test = None):\n",
        "\n",
        "        length = X_test.shape[0]\n",
        "        self.classes = [list(self.classes)] * len(X_test)\n",
        "\n",
        "        f = IntProgress(min=0, max=length) # instantiate the bar\n",
        "        display(f) # display the bar\n",
        "\n",
        "        probs_list = []\n",
        "\n",
        "        for clf in self.classifiers:\n",
        "            probs_list.append(clf.predict_proba(X_test)) \n",
        "\n",
        "        for i in range(length):\n",
        "              n_iter = 0\n",
        "              if debug: print(i, \"th istance\\n\", test_instance['metadata'])\n",
        "              while(len(self.classes[i]) > 1 and n_iter < max_iterations):\n",
        "                  n_iter = n_iter + 1\n",
        "                  if debug: print(\"iterazione: \", n_iter)\n",
        "                  max_variance, index, prob_best = 0, 0, []\n",
        "                  for j, _ in enumerate(self.classifiers):\n",
        "                      probs = probs_list[j][i]\n",
        "                      restricted_probs = [probs[m] for m in self.classes[i]]\n",
        "                      variance = max(restricted_probs)-min(restricted_probs)\n",
        "                      #variance = np.var(restricted_probs)\n",
        "                      #variance = entropy(restricted_probs)\n",
        "                      if debug: print(\"classifier \", clf.__class__.__name__, \" has variance \", variance)\n",
        "                      if variance >= max_variance:\n",
        "                          max_variance = variance\n",
        "                          index = j\n",
        "                          prob_best = probs\n",
        "                  self.classes[i] = self.survival(prob_best, self.classes[i])\n",
        "                  if debug: print(\"selected classes: \", self.classes[i])\n",
        "                  if debug and y_test: print(\"true class is \", y_test[i], \"\\n\")\n",
        "                  if (n_iter == max_iterations):\n",
        "                      self.classes[i] = nlargest(1, self.classes[i], key = lambda x : prob_best[x])\n",
        "              f.value += 1\n",
        "\n",
        "              \n",
        "\n",
        "        return self.classes\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \n",
        "        self.classes = [list(self.classes)] * len(X_test)\n",
        "        for j, clf in enumerate(self.classifiers):\n",
        "            probs = clf.predict_proba(X_test)\n",
        "            if (self.tuning):\n",
        "                X_test = self.ftuning(X_test, probs)\n",
        "            # for each instance of the test set, we filter the clases with SoF\n",
        "            for i, series_probs in enumerate(probs):\n",
        "                    \n",
        "                # if the classifier is the last one in the pipeline, we automatically select the most probable class\n",
        "                if (j == (len(self.classifiers)-1)):\n",
        "                    self.classes[i] = nlargest(1, self.classes[i], key = lambda x : series_probs[x])\n",
        "                    \n",
        "                #sof filtering\n",
        "                elif (self.criterion == 'sof'):\n",
        "                    self.classes[i] = self.survival(series_probs, self.classes[i])\n",
        "                #topk filtering\n",
        "                elif (self.criterion == 'topk'):\n",
        "                    k = self.params['k'][j]\n",
        "                    self.classes[i] = nlargest(k, self.classes[i], key = lambda x : series_probs[x])\n",
        "                #qf filtering\n",
        "                elif (self.criterion == 'qf'):\n",
        "                    k = int(len(self.classes[i])*self.params['q'])\n",
        "                    if (k == 0): k = 1\n",
        "                    self.classes[i] = nlargest(k, self.classes[i], key = lambda x : series_probs[x])\n",
        "            \n",
        "        return self.classes  \n",
        "    \n",
        "\n",
        "\n",
        "    def ftuning(self, X_test, probs):\n",
        "        for prob_list, (_, row) in zip(probs, X_test.iterrows()):\n",
        "            row['statistics'] = row['statistics'][:-len(prob_list)]\n",
        "            row['statistics'] = np.append(row['statistics'], prob_list) \n",
        "            \n",
        "        return X_test\n",
        "    \n",
        "    def accuracy(self, classes, y_test):\n",
        "        y_pred = []\n",
        "        for pred in classes:\n",
        "            y_pred.append(*pred)\n",
        "            \n",
        "        return accuracy_score(y_pred, y_test)*100\n",
        "    \n",
        "    def f1_score_macro(self, classes, y_test):\n",
        "        y_pred = []\n",
        "        for pred in classes:\n",
        "            y_pred.append(*pred)\n",
        "        \n",
        "        return f1_score(y_pred, y_test, average=\"macro\") * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7C5eqOxh4JNa"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R9b6Q9EU4JNa"
      },
      "source": [
        "Let's prepare our data so that it is compatible with our EnsembleFramework class. We extract the input data in a dictionary with keys 'statistics', 'metadata', 'timeseries', one for each algorithm type (respectively, BOS, NLP and TSC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# label_dict = {\n",
        "#                 \"appliance temperature\": 0,\n",
        "#                 \"humidity\": 1,\n",
        "#                 \"pressure\": 2,\n",
        "#                 \"indoor air temperature\": 3,\n",
        "#                 \"outdoor air temperature\": 4,\n",
        "#                 \"wind speed\": 5,\n",
        "#                 \"light\": 6,\n",
        "#                 \"air quality\": 7,\n",
        "#                 \"wind direction\": 8,\n",
        "#                 \"voltage\": 9,\n",
        "#                 \"current intensity\": 10,\n",
        "#                 \"wireless RSSI\": 11,\n",
        "#                 \"heat index\": 12,\n",
        "#                 \"dewpoint\": 13,\n",
        "#                 \"rain index\": 14,\n",
        "#                 \"PM 1\": 15,\n",
        "#                 \"PM 2.5\": 16,\n",
        "#                 \"PM 10\": 17,\n",
        "#                 \"CO2\": 18,\n",
        "#             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "### SensEUR Labels\n",
        "# label_dict = {\n",
        "#                 \"Atmospheric pressure\": 0,\n",
        "#                 \"Ambient Temperature\": 1,\n",
        "#                 \"Ambient Relative humidity\": 2,\n",
        "#                 \"Internal Temperature\": 3,\n",
        "#                 \"Internal Relative humidity\": 4,\n",
        "#                 \"CO\": 5,\n",
        "#                 \"CO2\": 6,\n",
        "#                 \"NO\": 7,\n",
        "#                 \"NO2\": 8,\n",
        "#                 \"PM10\": 9,\n",
        "#                 \"PM2.5\": 10,\n",
        "#                 \"PM1\": 11,\n",
        "#             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Dd6BDx774JNb"
      },
      "outputs": [],
      "source": [
        "def import_dataset(name):\n",
        "    y = []\n",
        "    X = {'metadata': [], 'statistics' : [], 'timeseries' : []}\n",
        "    # csv_file = f'{WORK_DIR}' + '/Datasets/' + name + '.csv'\n",
        "    scaler = StandardScaler()\n",
        "    # df = pd.read_csv(\"/home/muhammadinan/Documents/GitHub/Multi-modal-Journal-Paper/datasets/new thingspeak/Thingspeak_NEW.csv\", encoding=\"latin-1\")\n",
        "    # mask_value = 0.0000000000000001  # Value to replace None with\n",
        "    # label_dict = {\"co2\": 0, \"humidity\": 1, \"light\": 2, \"pir\": 3, \"temperature\": 4}\n",
        "    # df = pd.read_csv(\"/home/muhammadinan/Documents/GitHub/IOTTimeseriesOnly/datasets/smart_buildings_data_cleaned.csv\")\n",
        "    \n",
        "    # df = pd.read_csv(\n",
        "    #             \"/home/muhammadinan/Documents/GitHub/Multi-modal-Journal-Paper/datasets/ThingSpeakEU.csv\")\n",
        "    # IOWA\n",
        "    df = pd.read_csv(\"/home/muhammadinan/Documents/GitHub/IoT_Classification/Datasets/IOWA_PROCESSED.csv\")\n",
        "    # SensEUR\n",
        "    # df = pd.read_excel(\"/home/muhammadinan/Documents/GitHub/IoT_Classification/Datasets/SensEUR_Cleaned_NEW.xlsx\")\n",
        "    \n",
        "    # df[\"label\"] = df[\"label\"].replace(label_dict)\n",
        "    # mask_value = 0.0000000000000001\n",
        "    # df = df.fillna(mask_value)\n",
        "\n",
        "    import math\n",
        "    for index, row in df.iterrows():\n",
        "    # with open(csv_file, 'r') as dati:\n",
        "        # for row in dati:\n",
        "        # riga = row.strip().split(',')\n",
        "        # print(riga.count(\"None\"))\n",
        "        riga = row.values.tolist()\n",
        "        # print(riga)\n",
        "        # break\n",
        "        # riga = [mask_value if item == \"None\" else item for item in riga]\n",
        "        # print(len(riga))\n",
        "        # break\n",
        "        classe = int(riga[0])\n",
        "        y.append(classe)\n",
        "        # print(classe)\n",
        "        # print(label_dict[classe])\n",
        "        valori = np.array(riga[1:]).astype(\"float32\")\n",
        "        # print(np.count_nonzero(np.isnan(valori)))\n",
        "        # valori = np.nan_to_num(valori,nan=mask_value)\n",
        "    \n",
        "        # print(valori)\n",
        "        X['timeseries'].append(list(valori))\n",
        "        \n",
        "        # # metadata\n",
        "        stream_name = \"NO METADATA\" # riga[0]\n",
        "        X['metadata'].append(stream_name)\n",
        "        \n",
        "        # statistical features\n",
        "        media = np.mean(valori)\n",
        "        mediana = np.median(valori)\n",
        "        maxim = np.max(valori)\n",
        "        minim = np.min(valori)\n",
        "        std_dev = np.std(valori)\n",
        "        rms = np.sqrt(np.mean(np.square(valori)))\n",
        "        quantile = np.quantile(valori, 0.4)\n",
        "        i_q_r = iqr(valori)\n",
        "        import math\n",
        "        simmetria = skew(valori)\n",
        "        if math.isnan(simmetria):\n",
        "            simmetria = 0\n",
        "        curtosi = kurtosis(valori)\n",
        "        if math.isnan(curtosi):\n",
        "            curtosi = 0\n",
        "        rang = maxim - minim\n",
        "        \n",
        "        features = [rang, maxim, std_dev, rms, media, minim, quantile, mediana, curtosi, simmetria, i_q_r] \n",
        "        X['statistics'].append(features)\n",
        "\n",
        "    # X['timeseries'] = list(scaler.fit_transform(X['timeseries']))\n",
        "    X = pd.DataFrame(X)\n",
        "\n",
        "    # split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 100)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.26.1'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JFjoQc93wv7_"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = import_dataset('SensEUR-NEW')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2RBThxWZ4JNd"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CFsS8oKN4JNe"
      },
      "source": [
        "EnsembleFramework inizialization example.\n",
        "\n",
        "\n",
        "To insert a NLP type classifier, we need to insert the tag 'type' = 'NLP'.\n",
        "\n",
        "To insert a TSC classifier, we can pass the class TSC_1NN and as argument of that class the function used as distance (such as distance.euclidean).\n",
        "\n",
        "To insert a BOS classifier, we need to insert in the 'name' tag the constructor call of the classifier we want to use. Here we can use any scikit-learn classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HXQ_LhLL4JNe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  87.33333333333333\n",
            "F1 Macro :  87.57174353343275\n"
          ]
        }
      ],
      "source": [
        "clf = EnsembleFramework(criterion = 'sof', layers = [\n",
        "                                                    # {'type' : 'NLP', 'name' : 'NLP_Classifier()'},\n",
        "                                                     {'type' : 'BOS', 'name' : 'SVC(probability=True, random_state=100)'},\n",
        "                                                     {'type' : 'BOS', 'name' : 'GradientBoostingClassifier(random_state=100)'},\n",
        "                                                      {'type' : 'TSC', 'name' : 'TSC_1NN(metric = distance.euclidean)'},\n",
        "                                                     {'type' : 'BOS', 'name' : 'RandomForestClassifier(1000, random_state=100)'},\n",
        "                                                    #  {'type' : 'BOS', 'name' : 'GradientBoostingClassifier(random_state=100)'},\n",
        "                                                    #,\n",
        "                                                    #  {'type' : 'BOS', 'name' : 'KNeighborsClassifier()'},\n",
        "                                                    #  {'type' : 'BOS', 'name' : 'SVC(probability=True, random_state=100)'},\n",
        "                                                    #  {'type' : 'TSC', 'name' : 'TSC_1NN(metric = distance.euclidean)'},\n",
        "                                                    #  {'type' : 'BOS', 'name' : 'RandomForestClassifier(1000, random_state=100)'}\n",
        "                                                     ],\n",
        "                        params = {'sigma' : -0.6})\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy : \", clf.accuracy(y_pred, y_test))\n",
        "print(\"F1 Macro : \", clf.f1_score_macro(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "le2_7uKGx1Py"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waoym3JPyCAE"
      },
      "source": [
        "k_accuracy on the training set with Stratified 5-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rAVf8cUlnCbA"
      },
      "outputs": [],
      "source": [
        "def k_accuracy_cv(clf, X_train, y_train):\n",
        "    skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "    acc = []\n",
        "    n_classes = len(set(y_train))+1\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    k_acc = defaultdict(list)\n",
        "      \n",
        "    for train_index, test_index in skf.split(X_train, y_train):\n",
        "        X_train_loo, X_test_loo = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
        "        y_train_loo, y_test_loo = y_train[train_index], y_train[test_index]\n",
        "        clf.fit(X_train_loo, y_train_loo)\n",
        "        y_proba = clf.predict_proba(X_test_loo)\n",
        "        for k in range(1, n_classes):\n",
        "            for pred, test in zip(y_proba, y_test_loo):\n",
        "                classes = nlargest(k, list(set(y_train)), key = lambda x : pred[x])\n",
        "                if (test in classes):\n",
        "                    k_acc[k].append(1)\n",
        "                else:\n",
        "                    k_acc[k].append(0)\n",
        "\n",
        "    for k in range(1, n_classes):\n",
        "        acc.append(np.mean(k_acc[k])*100)\n",
        "    \n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fjddLIXayS_N"
      },
      "source": [
        "GridSearch CV to select the optimal value of sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lokiDrysCBTw"
      },
      "outputs": [],
      "source": [
        "def sigma_grid_search_cv(ensemble, X_train, y_train, sigmas, cv = 5):\n",
        "    parameters = {'params': [{'sigma' : s} for s in sigmas]}\n",
        "    \n",
        "    gs = GridSearchCV(ensemble, parameters, scoring = 'accuracy', cv = StratifiedKFold(cv))\n",
        "    gs.fit(X_train, y_train)\n",
        "\n",
        "    return gs.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4D_D6ODCyc5E"
      },
      "source": [
        "GridSearch CV to select the optimal value of q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TpWenKbpn0Se"
      },
      "outputs": [],
      "source": [
        "def q_grid_search_cv(ensemble, X_train, y_train, qs, cv = 5):\n",
        "    parameters = {'params': [{'q' : q} for q in qs]}\n",
        "    \n",
        "    gs = GridSearchCV(ensemble, parameters, scoring = 'accuracy', cv = StratifiedKFold(cv))\n",
        "    gs.fit(X_train, y_train)\n",
        "\n",
        "    return gs.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7vBs1h8mygme"
      },
      "source": [
        "Converts a dataframe to a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nWCfwrP89pVT"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_dict(df):\n",
        "    df.rename(columns={'Unnamed: 0':'input_classifiers'}, inplace=True)\n",
        "    dict_ = defaultdict(lambda: defaultdict(list))\n",
        "    for i, r in df.iterrows():\n",
        "        input_ = str(r['input_classifiers'])\n",
        "\n",
        "        if (isinstance(r['accuracies'], str)):\n",
        "          dict_[input_]['accuracies'] = eval(r['accuracies'])\n",
        "        else:\n",
        "          dict_[input_]['accuracies'] = []\n",
        "        \n",
        "        if (isinstance(r['permutation_specific_accuracies'], str)):\n",
        "          dict_[input_]['permutation_specific_accuracies'] = eval(r['permutation_specific_accuracies'])\n",
        "        else:\n",
        "          dict_[input_]['permutation_specific_accuracies'] = []\n",
        "        \n",
        "        if (isinstance(r['accuracy'], str)):\n",
        "          dict_[input_]['accuracy'] = eval(r['accuracy'])\n",
        "        else:\n",
        "          dict_[input_]['accuracy'] = r['accuracy']\n",
        "\n",
        "        if (isinstance(r['best_ordering'], str)):\n",
        "          dict_[input_]['best_ordering'] = eval(r['best_ordering'])\n",
        "        else:\n",
        "          dict_[input_]['best_ordering'] = []\n",
        "\n",
        "        dict_[input_]['sigma'] = r['sigma']\n",
        "\n",
        "    return dict_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1izx8E-ysq9"
      },
      "source": [
        "Selection criterion based on k_accuracy values.\n",
        "\n",
        "Starting from k = 1, we select the classifier with higher k_accuracy.\n",
        "\n",
        "Once a classifier is inserted, it cannot reappear in the pipeline.\n",
        "\n",
        "k values are the points in which the highest k_accuracy classifier changes.\n",
        "\n",
        "Output: \n",
        "- array con classifier index;\n",
        "- array with the values of k.\n",
        "\n",
        "EXAMPLE\n",
        "- output clf_index = [0, 0, 2, 2, 1, 1, ... , 1]\n",
        "- output k = [5, 3, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2iKke8Qhr7H6"
      },
      "outputs": [],
      "source": [
        "def select_from_k_accuracy(accuracies, n_classes):\n",
        "    clf_index = []\n",
        "    accs = accuracies.copy()\n",
        "    for n in range(0, n_classes):\n",
        "        k_accuracies = [acc[n] for acc in accs]\n",
        "        \n",
        "        if (n > 0 and max(k_accuracies) == k_accuracies[clf_index[n-1]]):\n",
        "            clf_index.append(clf_index[n-1])\n",
        "            if (n < n_classes-1):\n",
        "                k1_accuracies = [acc[n+1] for acc in accs]\n",
        "                if (max(k1_accuracies) != k1_accuracies[clf_index[n]]):\n",
        "                    accs[clf_index[n]] = [0 for x in accs[clf_index[n]]]\n",
        "        else:\n",
        "            index = np.argmax(k_accuracies)\n",
        "            clf_index.append(index)\n",
        "\n",
        "    #for n in np.arange(n_classes-2, 0, -1):\n",
        "        #k_accuracies = [acc[n] for acc in accuracies]\n",
        "        #if (k_accuracies[clf_index[n]] == k_accuracies[clf_index[n+1]]):\n",
        "            #clf_index[n] = clf_index[n+1]\n",
        "\n",
        "    k = []\n",
        "    for n in np.arange(n_classes-1, 0, -1):\n",
        "        if (clf_index[n] != clf_index[n-1]):\n",
        "            k.append(n+1)\n",
        "    if (clf_index[0] != clf_index[1]):\n",
        "        k.append(1)\n",
        "    elif (not k or k[-1] != 1):\n",
        "        k.append(1)\n",
        "\n",
        "    return clf_index, k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o37tGkLK0A5n"
      },
      "source": [
        "Starting from a list of indexes and a list of classifiers, returns the list of the chosen classifiers.\n",
        "\n",
        "EXAMPLE\n",
        "- index_list = [0, 0, 0, 2, 2, 1]\n",
        "- clf_list = [RF, SVM, kNN, NLP]\n",
        "\n",
        "- output = [RF, kNN, SVM]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5-G_8ulT7VLy"
      },
      "outputs": [],
      "source": [
        "def select_from_index(index_list, clf_list):\n",
        "    selected_clf_list = []\n",
        "    for i in np.arange(len(index_list)-1, -1, -1):\n",
        "        if (i == len(index_list)-1):\n",
        "            selected_clf_list.append(clf_list[index_list[i]])\n",
        "        elif (index_list[i] != index_list[i+1]):\n",
        "            selected_clf_list.append(clf_list[index_list[i]])\n",
        "    \n",
        "    return selected_clf_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IZuapGYyx74w"
      },
      "source": [
        "\n",
        "# Validation test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ORPurM4usLpJ"
      },
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "puMUElTLyK21"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from itertools import permutations\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "clfs = []\n",
        "clfs.append({'type' : 'TSC', 'name' : 'TSC_1NN(metric = distance.euclidean)'})\n",
        "clfs.append({'type' : 'BOS', 'name' : 'KNeighborsClassifier()'})\n",
        "# clfs.append({'type' : 'NLP', 'name' : 'NLP_Classifier()'})\n",
        "clfs.append({'type' : 'BOS', 'name' : 'GradientBoostingClassifier(random_state=100)'})\n",
        "clfs.append({'type' : 'BOS', 'name' : 'RandomForestClassifier(1000, random_state=100)'})\n",
        "clfs.append({'type' : 'BOS', 'name' : 'SVC(probability=True, random_state=100)'})\n",
        "\n",
        "sigmas = np.arange(-0.6, 1.2, 0.2)\n",
        "qs = np.arange(0.1, 1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'type': 'TSC', 'name': 'TSC_1NN(metric = distance.euclidean)'}, {'type': 'BOS', 'name': 'KNeighborsClassifier()'}, {'type': 'BOS', 'name': 'GradientBoostingClassifier(random_state=100)'}, {'type': 'BOS', 'name': 'RandomForestClassifier(1000, random_state=100)'}, {'type': 'BOS', 'name': 'SVC(probability=True, random_state=100)'}]\n"
          ]
        }
      ],
      "source": [
        "print(clfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V5h8jTEs-Huz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total iterations: 2925\n"
          ]
        }
      ],
      "source": [
        "max_count = 0\n",
        "results = defaultdict(lambda: defaultdict(list))\n",
        "# dati tutti i subset possibili di classificatori\n",
        "for i in range(len(clfs), len(clfs)+1):\n",
        "\n",
        "    # date tutte le possibili combinazioni messe in input\n",
        "    combs = list(combinations(clfs, i))\n",
        "    for comb in combs:\n",
        "        for j in range(1, len(comb)+1):\n",
        "            sub_combs = list(combinations(comb, j))\n",
        "            for sub_comb in sub_combs:\n",
        "                sub_names = str(sub_comb)\n",
        "                if (results[sub_names]['accuracy']):\n",
        "                    pass\n",
        "                else:\n",
        "                    perms = list(permutations(sub_comb))\n",
        "                    for perm in perms:\n",
        "                        perm_name = str(perm)\n",
        "                        for en, s in enumerate(sigmas):\n",
        "                            if (len(results[perm_name]['permutation_specific_accuracies']) > en):\n",
        "                                acc = results[perm_name]['permutation_specific_accuracies'][en]\n",
        "                            else:\n",
        "                                # results[perm_name]['permutation_specific_accuracies'].append(0)\n",
        "                                max_count = max_count + 1\n",
        "        classifier_names = str(comb)\n",
        "        # results[classifier_names]['accuracy'] = 0\n",
        "        # results[classifier_names]['best_ordering'] = ['a', 'b']\n",
        "\n",
        "print('Total iterations: {}'.format(max_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NSe644-cthMl"
      },
      "source": [
        "\n",
        "We compute the optimal set of classifiers for every possibile combination of classifiers given in input.\n",
        "We use the SoF criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "c3184223b775434cbefcb14d649e2850",
            "5b9a0b1b75a443b3970ea8ab636ee145",
            "e5ce3e4ce99b419eb541cd47f393e7b3"
          ]
        },
        "colab_type": "code",
        "id": "NaZFe6yy_vhT",
        "outputId": "7512a56d-5de5-4b5a-cf02-8e1ede0c12d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f4ae19f61f34d2fa14a6bf06c4a22e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "IntProgress(value=0, max=2925)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model given ['TSC_1NN', 'KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 'SVC'] classifiers as input is ['TSC_1NN', 'GradientBoostingClassifier'] with accuracy of: 91.000 and sigma 0.8\n"
          ]
        }
      ],
      "source": [
        "# results = pd.read_csv(f'{WORK_DIR}thingspeak_combinations_final.csv')\n",
        "# results = dataframe_to_dict(results)\n",
        "\n",
        "f = IntProgress(min=0, max=max_count)\n",
        "display(f)\n",
        "\n",
        "# dati tutti i subset possibili di classificatori\n",
        "for i in range(len(clfs), len(clfs)+1):\n",
        "\n",
        "    # date tutte le possibili combinazioni messe in input\n",
        "    combs = list(combinations(clfs, i))\n",
        "    for comb in combs:\n",
        "        classifier_names = str(comb)\n",
        "\n",
        "        best_ordering, best_sigma, best_accuracy = [], 0, 0\n",
        "        for j in range(1, len(comb)+1):\n",
        "\n",
        "            # date tutte le possibili sottocombinazioni\n",
        "            sub_combs = list(combinations(comb, j))  \n",
        "            for sub_comb in sub_combs:\n",
        "                # controlliamo se abbiamo giÃ  il risultato per quella sottocombinazione\n",
        "                sub_names = str(sub_comb)\n",
        "                if (results[sub_names]['accuracy']):\n",
        "                    acc = results[sub_names]['accuracy']\n",
        "                    results[classifier_names]['accuracies'].extend(results[sub_names]['accuracies'])\n",
        "\n",
        "                    if (acc > best_accuracy):\n",
        "                        best_ordering = results[sub_names]['best_ordering']\n",
        "                        best_sigma = results[sub_names]['sigma']\n",
        "                        best_accuracy = acc\n",
        "                else: \n",
        "                    # controlliamo tutti i possibili ordinamenti dati in input\n",
        "                    perms = list(permutations(sub_comb))\n",
        "                    for perm in perms:\n",
        "                        layers = [x for x in perm]\n",
        "                        perm_name = str(perm)\n",
        "                        # e tutti i possibili sigma\n",
        "                        for en, s in enumerate(sigmas):\n",
        "\n",
        "                            if (len(results[perm_name]['permutation_specific_accuracies']) > en):\n",
        "                                acc = results[perm_name]['permutation_specific_accuracies'][en]\n",
        "                            else:\n",
        "                                ensemble = EnsembleFramework(criterion = 'sof', layers = layers, params = {'sigma' : s})\n",
        "                                ensemble.fit(X_train, y_train)\n",
        "                                y_pred = ensemble.predict(X_test)\n",
        "                                acc = ensemble.accuracy(y_pred, y_test)\n",
        "\n",
        "                                results[classifier_names]['accuracies'].append(acc)\n",
        "                                results[perm_name]['permutation_specific_accuracies'].append(acc)\n",
        "                                f.value += 1\n",
        "\n",
        "                            if (acc > best_accuracy):\n",
        "                                best_ordering = layers\n",
        "                                best_sigma = s\n",
        "                                best_accuracy = acc\n",
        "                            \n",
        "                            # thingspeak_results = pd.DataFrame.from_dict(results, orient='index')\n",
        "                            # thingspeak_results.to_csv(f'{WORK_DIR}zagreb_new_combinations_final.csv')\n",
        "\n",
        "        results[classifier_names]['best_ordering'] = best_ordering\n",
        "        results[classifier_names]['accuracy'] = best_accuracy\n",
        "        results[classifier_names]['sigma'] = best_sigma\n",
        "        formatted_names = [re.sub(r'\\([^()]*\\)', '', x['name']) for x in comb]\n",
        "        formatted_best = [re.sub(r'\\([^()]*\\)', '', x['name']) for x in best_ordering]\n",
        "        print(\"Best model given {} classifiers as input is {} with accuracy of: {:.3f} and sigma {:.1f}\".format(formatted_names, \n",
        "                                                                                                                formatted_best, \n",
        "                                                                                                                best_accuracy,\n",
        "                                                                                                                best_sigma))\n",
        "\n",
        "# thingspeak_results = pd.DataFrame.from_dict(results, orient='index')\n",
        "# thingspeak_results.to_csv(f'{WORK_DIR}thingspeak_combinations_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GxYgmDGsQzV"
      },
      "source": [
        "## Comparison between brute-force approach and our heuristic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SSdHfUgPwKa8"
      },
      "source": [
        "\n",
        "\n",
        "Let's retrieve our data with the combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H1UfnRaJFPs7"
      },
      "outputs": [],
      "source": [
        "# results = pd.read_csv(f'{WORK_DIR}' + '/CombinationsData/thingspeak_combinations_final.csv')\n",
        "# results = dataframe_to_dict(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oQLIHvB1uDKR"
      },
      "source": [
        "For every combination of classifier in input, we compare our heuristics to the brute-force approach.\n",
        "\n",
        "Our heuristics:\n",
        "\n",
        "\n",
        "*   k-heuristic: the heuristic selects the classifiers, their order and the values of k. TopK criterion\n",
        "*   sigma-heuristic: the heuristic only selects the classifier and their order. A GridSearch with Cross Validation then selects the optimal value of sigma. SoF criterion\n",
        "*   qf-heuristic: similar to the one above. QF criterion\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IE-AkVtc5_Di"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combinations number: 26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f94c8b8216c84ab2904240ad622bcfb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "IntProgress(value=0, max=26)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'GradientBoostingClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'RandomForestClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'SVC']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'GradientBoostingClassifier']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'SVC']\n",
            "Analyzing combination  ['GradientBoostingClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['GradientBoostingClassifier', 'SVC']\n",
            "Analyzing combination  ['RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'GradientBoostingClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'GradientBoostingClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'GradientBoostingClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'GradientBoostingClassifier', 'SVC']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['GradientBoostingClassifier', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'GradientBoostingClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'GradientBoostingClassifier', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 'SVC']\n",
            "Analyzing combination  ['TSC_1NN', 'KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 'SVC']\n"
          ]
        }
      ],
      "source": [
        "tot_combinations = 0\n",
        "\n",
        "for i in range(2,len(clfs)+1):\n",
        "\n",
        "    combs = list(combinations(clfs, i))\n",
        "    for comb in combs:\n",
        "        tot_combinations = tot_combinations+1\n",
        "\n",
        "print('Combinations number: {}'.format(tot_combinations))\n",
        "\n",
        "f = IntProgress(min=0, max=tot_combinations)\n",
        "display(f)\n",
        "\n",
        "# all_combinations_iqr = []\n",
        "# all_combinations_std = []\n",
        "\n",
        "# k_heuristic_deviation = []\n",
        "# sigma_heuristic_deviation = []\n",
        "# qf_heuristic_deviation = []\n",
        "\n",
        "# k_heuristic_absolute_deviation = []\n",
        "# sigma_heuristic_absolute_deviation = []\n",
        "# qf_heuristic_absolute_deviation = []\n",
        "\n",
        "# k_heuristic_percentile = []\n",
        "# sigma_heuristic_percentile = []\n",
        "# qf_heuristic_percentile = []\n",
        "\n",
        "topk_accuracies = []\n",
        "topk_f1_score = []\n",
        "sof_accuracies = []\n",
        "sof_f1_score = []\n",
        "qf_accuracies = []\n",
        "qf_f1_score = []\n",
        "\n",
        "# dati tutti i subset possibili di classificatori\n",
        "for i in range(2,len(clfs)+1):\n",
        "\n",
        "    # date tutte le possibili combinazioni messe in input\n",
        "    combs = list(combinations(clfs, i))\n",
        "    for comb in combs:\n",
        "        classifier_names = str(comb)\n",
        "        \n",
        "        accuracy_distributions = []\n",
        "        for input_clf in comb:\n",
        "            if (input_clf['type'] == 'BOS'):\n",
        "                clf = BOS_Classifier(eval(input_clf['name']))\n",
        "            elif (input_clf['type'] == 'NLP'):\n",
        "                clf = NLP_Classifier()\n",
        "            elif (input_clf['type'] == 'TSC'):\n",
        "                clf = eval(input_clf['name'])\n",
        "            accuracy_distributions.append(k_accuracy_cv(clf, X_train, y_train))\n",
        "\n",
        "        index_list, k = select_from_k_accuracy(accuracy_distributions, len(set(y_test)))\n",
        "        selected_classifiers = select_from_index(index_list, comb)\n",
        "\n",
        "        formatted_names = [re.sub(r'\\([^()]*\\)', '', x['name']) for x in comb]\n",
        "        print(\"Analyzing combination \", formatted_names)\n",
        "\n",
        "        # usiamo l'heuristica sia per la selezione e l'ordinamento sia per ordinare il k\n",
        "        ensemble = EnsembleFramework(criterion = 'topk', layers = selected_classifiers, params = {'k': k})\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "        k_heuristic_accuracy = ensemble.accuracy(y_pred, y_test)\n",
        "        k_heuristic_f1_score = ensemble.f1_score_macro(y_pred, y_test)\n",
        "\n",
        "        topk_accuracies.append(k_heuristic_accuracy)\n",
        "        topk_f1_score.append(k_heuristic_f1_score)\n",
        "\n",
        "        # print(\"Accuracy with heuristic and topk {:.3f}\".format(k_heuristic_accuracy))\n",
        "\n",
        "        # all_accuracies = results[classifier_names]['accuracies']\n",
        "        # all_combinations_iqr.append(iqr(all_accuracies))\n",
        "        # all_combinations_std.append(np.std(all_accuracies))\n",
        "\n",
        "        # print(\"All accuracies obtainable with this combination: \", all_accuracies)\n",
        "\n",
        "        # k_heuristic_deviation.append(k_heuristic_accuracy/max(all_accuracies)*100)\n",
        "        # k_heuristic_percentile.append(percentileofscore(all_accuracies, k_heuristic_accuracy))\n",
        "        # k_heuristic_absolute_deviation.append(k_heuristic_accuracy-max(all_accuracies))\n",
        "\n",
        "        # print(\"k-heuristic percentage: \", k_heuristic_accuracy/max(all_accuracies)*100)\n",
        "        # print(\"k-heuristic percentile: \", percentileofscore(all_accuracies, k_heuristic_accuracy))\n",
        "\n",
        "        # usiamo l'heuristica solo per la selezione e l'ordinamento, simuliamo una cross-validation per trovare sigma\n",
        "\n",
        "        ensemble = EnsembleFramework(criterion = 'sof', layers = selected_classifiers)\n",
        "        ensemble = sigma_grid_search_cv(ensemble, X_train, y_train, sigmas)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "        sigma_heuristic_accuracy = ensemble.accuracy(y_pred, y_test)\n",
        "        sigma_heuristic_f1_score = ensemble.f1_score_macro(y_pred, y_test)\n",
        "\n",
        "        sof_accuracies.append(sigma_heuristic_accuracy)\n",
        "        sof_f1_score.append(sigma_heuristic_f1_score)\n",
        "        # print(\"Accuracy with heuristic and cross-validation with sof {:.3f}\".format(sigma_heuristic_accuracy))\n",
        "        \n",
        "        # sigma_heuristic_deviation.append(sigma_heuristic_accuracy/max(all_accuracies)*100)\n",
        "        # sigma_heuristic_percentile.append(percentileofscore(all_accuracies, sigma_heuristic_accuracy))\n",
        "        # sigma_heuristic_absolute_deviation.append(sigma_heuristic_accuracy-max(all_accuracies))\n",
        "\n",
        "        # usiamo la seconda heuristica per la selezione e l'ordinamento, simuliamo una cross-validation per trovare sigma\n",
        "        \n",
        "        ensemble = EnsembleFramework(criterion = 'qf', layers = selected_classifiers)\n",
        "        ensemble = q_grid_search_cv(ensemble, X_train, y_train, qs)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "        qf_heuristic_accuracy = ensemble.accuracy(y_pred, y_test)\n",
        "        qf_heuristic_f1_score = ensemble.f1_score_macro(y_pred, y_test)\n",
        "        \n",
        "        qf_accuracies.append(qf_heuristic_accuracy)\n",
        "        qf_f1_score.append(qf_heuristic_f1_score)\n",
        "\n",
        "        # print(\"Accuracy with heuristic and cross-validation with sof {:.3f}\".format(sigma_heuristic_accuracy))\n",
        "        \n",
        "        # qf_heuristic_deviation.append(qf_heuristic_accuracy/max(all_accuracies)*100)\n",
        "        # qf_heuristic_percentile.append(percentileofscore(all_accuracies, qf_heuristic_accuracy))\n",
        "        # qf_heuristic_absolute_deviation.append(qf_heuristic_accuracy-max(all_accuracies))\n",
        "\n",
        "        f.value += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Best Accuracy (SoF):  90.33333333333333\n",
            "Best F1 Score (SoF):  90.43860408991988\n"
          ]
        }
      ],
      "source": [
        "# Get the index of the maximum accuracy\n",
        "max_accuracy_index = sof_accuracies.index(max(sof_accuracies))\n",
        "\n",
        "# Use the index to retrieve the corresponding F1 score\n",
        "max_f1_score = sof_f1_score[max_accuracy_index]\n",
        "print(max_accuracy_index)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Accuracy (SoF): \", max(sof_accuracies))\n",
        "print(\"Best F1 Score (SoF): \", max_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Best Accuracy (TopK):  89.66666666666666\n",
            "Best F1 Score (TopK)):  89.81687616928943\n"
          ]
        }
      ],
      "source": [
        "# Get the index of the maximum accuracy\n",
        "max_accuracy_index = topk_accuracies.index(max(topk_accuracies))\n",
        "\n",
        "# Use the index to retrieve the corresponding F1 score\n",
        "max_f1_score = topk_f1_score[max_accuracy_index]\n",
        "print(max_accuracy_index)\n",
        "# Print the results\n",
        "print(\"Best Accuracy (TopK): \", max(topk_accuracies))\n",
        "print(\"Best F1 Score (TopK)): \", max_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Accuracy (PF):  89.66666666666666\n",
            "Best F1 Score (PF)):  89.81687616928943\n"
          ]
        }
      ],
      "source": [
        "# Get the index of the maximum accuracy\n",
        "max_accuracy_index =    qf_accuracies.index(max(qf_accuracies))\n",
        "\n",
        "# Use the index to retrieve the corresponding F1 score\n",
        "max_f1_score = qf_f1_score[max_accuracy_index]\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Accuracy (PF): \", max(qf_accuracies))\n",
        "print(\"Best F1 Score (PF)): \", max_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "colab_type": "code",
        "id": "OBr7iL8xK10B",
        "outputId": "564674da-4cc1-4a6b-c720-88b0f398f89b"
      },
      "outputs": [],
      "source": [
        "print('IQR of the accuracies we are comparing our results to: ', all_combinations_iqr)\n",
        "print('STD of the accuracies we are comparing our results to: ', all_combinations_std)\n",
        "\n",
        "print('% of the accuracy achieved by our k-heuristic', k_heuristic_deviation)\n",
        "print('absolute deviation of the accuracy achieved by our k-heuristic', k_heuristic_absolute_deviation)\n",
        "print('percentile rank of the accuracy achieved by our k-heuristic', k_heuristic_percentile)\n",
        "\n",
        "print('% of the accuracy achieved by our sigma-heuristic', sigma_heuristic_deviation)\n",
        "print('absolute deviation of the accuracy achieved by our sigma-heuristic', sigma_heuristic_absolute_deviation)\n",
        "print('percentile rank of the accuracy achieved by our sigma-heuristic', sigma_heuristic_percentile)\n",
        "\n",
        "print('% of the accuracy achieved by our qf-heuristic', qf_heuristic_deviation)\n",
        "print('absolute deviation of the accuracy achieved by our qf-heuristic', qf_heuristic_absolute_deviation)\n",
        "print('percentile rank of the accuracy achieved by our second qf-heuristic', qf_heuristic_percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "colab_type": "code",
        "id": "cEikJzz-Y_lU",
        "outputId": "347c4312-c0c6-4f8c-f62a-298709f3249d"
      },
      "outputs": [],
      "source": [
        "print('Mean IQR of the accuracies we are comparing our results to: ', np.mean(all_combinations_iqr))\n",
        "print('Mean STD of the accuracies we are comparing our results to: ', np.mean(all_combinations_std))\n",
        "\n",
        "print('Mean % of the accuracy achieved by our k-heuristic', np.mean(k_heuristic_deviation))\n",
        "print('Mean absolute deviation of the accuracy achieved by our k-heuristic', np.mean(k_heuristic_absolute_deviation))\n",
        "print('Mean percentile rank of the accuracy achieved by our k-heuristic', np.mean(k_heuristic_percentile))\n",
        "\n",
        "print('Mean % of the accuracy achieved by our sigma-heuristic', np.mean(sigma_heuristic_deviation))\n",
        "print('Mean absolute deviation of the accuracy achieved by our sigma-heuristic', np.mean(sigma_heuristic_absolute_deviation))\n",
        "print('Mean percentile rank of the accuracy achieved by our sigma-heuristic', np.mean(sigma_heuristic_percentile))\n",
        "\n",
        "print('Mean % of the accuracy achieved by our qf-heuristic', np.mean(qf_heuristic_deviation))\n",
        "print('Mean absolute deviation of the accuracy achieved by our qf-heuristic', np.mean(qf_heuristic_deviation))\n",
        "print('Mean percentile rank of the accuracy achieved by our second qf-heuristic', np.mean(qf_heuristic_percentile))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QBFk3Tr-6LN5"
      },
      "outputs": [],
      "source": [
        "# thingspeak\n",
        "\n",
        "topk_percentage = [95.0, 84.42028985507247, 94.5378151260504, 98.56557377049178, 84.42028985507247, 99.36575052854121, 99.58592132505176, 83.81294964028777, 95.03546099290779, 99.79253112033194, 82.7708703374778, 94.40993788819875, 98.36400817995909, 83.6624775583483, 95.03546099290779, 98.36400817995909, 83.5125448028674, 94.86725663716813, 99.3801652892562, 95.03546099290779, 82.7708703374778, 94.86725663716813, 98.36400817995909, 95.03546099290779, 94.86725663716813, 94.86725663716813]\n",
        "sigma_percentage = [98.12499999999999, 84.42028985507247, 94.5378151260504, 98.97540983606557, 84.42028985507247, 98.30866807610994, 99.58592132505176, 83.81294964028777, 99.822695035461, 99.79253112033194, 82.7708703374778, 97.51552795031056, 98.77300613496931, 83.6624775583483, 99.822695035461, 98.77300613496931, 83.5125448028674, 99.64601769911503, 99.3801652892562, 99.822695035461, 82.7708703374778, 99.64601769911503, 98.77300613496931, 99.822695035461, 99.64601769911503, 99.64601769911503]\n",
        "qf_percentage = [98.12499999999999, 84.42028985507247, 94.5378151260504, 99.79508196721312, 84.42028985507247, 99.36575052854121, 99.58592132505176, 83.81294964028777, 94.14893617021276, 99.79253112033194, 82.7708703374778, 97.51552795031056, 99.59100204498978, 83.6624775583483, 94.14893617021276, 99.59100204498978, 83.5125448028674, 93.98230088495575, 99.3801652892562, 94.14893617021276, 82.7708703374778, 93.98230088495575, 99.59100204498978, 94.14893617021276, 93.98230088495575, 93.98230088495575]\n",
        "\n",
        "topk_percentile = [60.130718954248366, 93.05555555555556, 22.265625, 73.30729166666667, 59.88562091503268, 68.62745098039215, 65.11437908496733, 90.66840277777777, 95.13888888888889, 77.12673611111111, 75.03459865559509, 36.21984974298142, 77.60478449980229, 88.52124183006536, 94.94553376906318, 83.55392156862744, 73.43317516805061, 95.16113088177146, 79.97726374060893, 94.90196078431373, 80.31525938189846, 96.18584437086092, 85.00114974245768, 96.23436783866954, 96.19619205298014, 96.8814670822821]\n",
        "sigma_percentile = [79.49346405228758, 93.05555555555556, 22.265625, 97.61284722222223, 59.88562091503268, 62.5, 65.11437908496733, 90.66840277777777, 98.65451388888889, 77.12673611111111, 75.03459865559509, 89.05199683669434, 97.61269276393831, 88.52124183006536, 99.34912854030502, 98.76089324618736, 73.43317516805061, 99.13503361012258, 79.97726374060893, 99.37091503267973, 80.31525938189846, 99.57988410596026, 98.88360007358352, 99.68295767039875, 99.59299116997792, 99.76630601695183]\n",
        "qf_percentile = [79.49346405228758, 93.05555555555556, 22.265625, 99.30555555555556, 59.88562091503268, 68.62745098039215, 65.11437908496733, 90.66840277777777, 95.13888888888889, 77.12673611111111, 75.03459865559509, 89.05199683669434, 99.56504547251878, 88.52124183006536, 94.68681917211329, 99.65141612200436, 73.43317516805061, 95.04250691973112, 79.97726374060893, 94.81481481481481, 80.31525938189846, 96.06719094922737, 99.78476821192054, 96.13625761147122, 96.14146431199411, 96.81728061791222]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O-Z-G4Banock"
      },
      "outputs": [],
      "source": [
        "# urban\n",
        "\n",
        "topk_percentage = [98.93617021276596, 98.27586206896551, 98.30508474576271, 96.7032967032967, 99.3006993006993, 100.0, 99.63898916967509, 100.0, 99.64788732394366, 99.3103448275862, 98.27586206896551, 98.30508474576271, 97.52650176678446, 98.30508474576271, 97.93103448275862, 98.30508474576271, 100.0, 98.95104895104895, 99.3103448275862, 99.3103448275862, 98.30508474576271, 97.93103448275862, 98.30508474576271, 98.30508474576271, 99.3103448275862, 98.30508474576271]\n",
        "sigma_percentage = [98.93617021276596, 98.27586206896551, 98.30508474576271, 96.7032967032967, 99.3006993006993, 100.0, 99.63898916967509, 100.0, 99.29577464788733, 100.0, 98.27586206896551, 98.30508474576271, 97.52650176678446, 98.30508474576271, 96.89655172413794, 98.30508474576271, 100.0, 98.6013986013986, 100.0, 100.0, 98.30508474576271, 96.89655172413794, 98.30508474576271, 98.30508474576271, 100.0, 98.30508474576271]\n",
        "qf_percentage = [98.58156028368793, 98.62068965517241, 98.30508474576271, 95.97069597069597, 99.3006993006993, 100.0, 99.63898916967509, 100.0, 98.94366197183099, 98.9655172413793, 98.62068965517241, 98.30508474576271, 97.52650176678446, 98.30508474576271, 97.58620689655172, 98.30508474576271, 100.0, 98.25174825174825, 98.9655172413793, 98.9655172413793, 98.30508474576271, 97.58620689655172, 98.30508474576271, 98.30508474576271, 98.9655172413793, 98.30508474576271]\n",
        "\n",
        "topk_percentile = [94.44444444444444, 86.45833333333333, 71.18055555555556, 86.11111111111111, 66.66666666666667, 80.0, 85.55555555555556, 79.51388888888889, 57.986111111111114, 59.72222222222222, 89.63963963963964, 80.63063063063063, 88.28828828828829, 83.22222222222223, 79.96296296296296, 84.4074074074074, 85.13513513513513, 63.96396396396396, 70.87087087087087, 73.81481481481481, 85.96244131455398, 82.03442879499218, 86.82316118935837, 88.90542328042328, 77.339593114241, 89.68022076130184]\n",
        "sigma_percentile = [94.44444444444444, 86.45833333333333, 71.18055555555556, 86.11111111111111, 66.66666666666667, 80.0, 85.55555555555556, 79.51388888888889, 53.81944444444444, 80.90277777777777, 89.63963963963964, 80.63063063063063, 88.28828828828829, 83.22222222222223, 63.22222222222222, 84.4074074074074, 85.13513513513513, 59.309309309309306, 85.88588588588588, 87.5925925925926, 85.96244131455398, 65.11737089201878, 86.82316118935837, 88.90542328042328, 89.2018779342723, 89.68022076130184]\n",
        "qf_percentile = [91.11111111111111, 93.40277777777777, 71.18055555555556, 83.33333333333333, 66.66666666666667, 80.0, 85.55555555555556, 79.51388888888889, 51.041666666666664, 59.375, 94.14414414414415, 80.63063063063063, 88.28828828828829, 83.22222222222223, 66.62962962962963, 84.4074074074074, 85.13513513513513, 56.15615615615616, 70.42042042042043, 72.77777777777777, 85.96244131455398, 69.29577464788733, 86.82316118935837, 88.90542328042328, 76.4945226917058, 89.68022076130184]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Zh--mGaJ-_Vh"
      },
      "outputs": [],
      "source": [
        "# swissex\n",
        "\n",
        "topk_percentage = [94.04761904761904, 94.25287356321837, 100.0, 87.67123287671234, 97.75280898876406, 100.0, 97.43589743589742, 100.0, 98.86363636363637, 100.0, 91.1111111111111, 100.0, 92.94117647058823, 100.0, 93.18181818181817, 98.87640449438203, 98.87640449438203, 97.75280898876406, 100.0, 100.0, 97.77777777777776, 91.1111111111111, 98.87640449438203, 98.87640449438203, 98.87640449438203, 97.77777777777776]\n",
        "sigma_percentage = [98.8095238095238, 96.55172413793103, 97.72727272727273, 100.0, 97.75280898876406, 100.0, 98.71794871794873, 100.0, 97.72727272727273, 100.0, 93.33333333333333, 97.72727272727273, 97.64705882352942, 97.72727272727273, 95.45454545454545, 96.62921348314609, 98.87640449438203, 96.62921348314609, 100.0, 100.0, 95.55555555555554, 93.33333333333333, 96.62921348314609, 96.62921348314609, 98.87640449438203, 95.55555555555554]\n",
        "qf_percentage = [95.23809523809524, 97.70114942528735, 100.0, 94.52054794520546, 97.75280898876406, 100.0, 98.71794871794873, 100.0, 98.86363636363637, 96.59090909090908, 94.44444444444443, 100.0, 94.11764705882355, 100.0, 96.59090909090908, 98.87640449438203, 98.87640449438203, 97.75280898876406, 96.59090909090908, 96.59090909090908, 97.77777777777776, 94.44444444444443, 98.87640449438203, 98.87640449438203, 95.5056179775281, 97.77777777777776]\n",
        "\n",
        "topk_percentile = [63.888888888888886, 36.111111111111114, 86.11111111111111, 61.111111111111114, 75.0, 88.88888888888889, 58.333333333333336, 86.11111111111111, 77.77777777777777, 87.5, 46.2962962962963, 92.5925925925926, 83.06878306878306, 91.53439153439153, 57.93650793650794, 91.53439153439153, 91.26984126984127, 83.33333333333333, 92.85714285714286, 90.21164021164022, 93.95424836601308, 63.11274509803921, 94.64869281045752, 93.4640522875817, 93.42320261437908, 95.07109004739337]\n",
        "sigma_percentile = [95.83333333333333, 38.888888888888886, 62.5, 100.0, 75.0, 88.88888888888889, 77.77777777777777, 86.11111111111111, 55.55555555555556, 87.5, 55.55555555555556, 77.51322751322752, 98.14814814814815, 46.560846560846564, 62.16931216931217, 80.42328042328042, 91.26984126984127, 67.19576719576719, 92.85714285714286, 90.21164021164022, 59.72222222222222, 69.36274509803921, 86.27450980392157, 63.27614379084967, 93.42320261437908, 70.38441284886783]\n",
        "qf_percentile = [63.888888888888886, 47.22222222222222, 86.11111111111111, 80.55555555555556, 75.0, 88.88888888888889, 77.77777777777777, 86.11111111111111, 77.77777777777777, 69.44444444444444, 62.16931216931217, 92.5925925925926, 85.44973544973546, 91.53439153439153, 67.19576719576719, 91.53439153439153, 91.26984126984127, 83.33333333333333, 79.8941798941799, 42.32804232804233, 93.95424836601308, 73.24346405228758, 94.64869281045752, 93.4640522875817, 55.14705882352941, 95.07109004739337]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "colab_type": "code",
        "id": "GySDmFsjj6mF",
        "outputId": "44d6bfb9-dd34-4dfb-f4aa-fe36a03dda22"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "f, ax = plt.subplots(figsize = (7,5))\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "#colors = [\"windows blue\", \"amber\", \"faded green\", \"dusty purple\"]\n",
        "sns.boxplot(data=[topk_percentage, sigma_percentage, qf_percentage], fliersize = 0, medianprops={'color':'red', 'linewidth': 3}, color='white')\n",
        "sns.swarmplot(data=[topk_percentage, sigma_percentage, qf_percentage], color=\".25\", size = 10)\n",
        "\n",
        "ax.set_xticklabels(['TKSE', 'SoF', 'PF'])\n",
        "#major_ticks = np.arange(85, 101, 5)\n",
        "\n",
        "#ax.set_yticks(major_ticks)\n",
        "ax.set(ylabel = 'Normalized Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "colab_type": "code",
        "id": "ehxfFkIXupdV",
        "outputId": "b7e67116-ca4c-4996-b285-d2b2271b620f"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize = (7,5))\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "colors = [\"windows blue\", \"amber\", \"faded green\", \"dusty purple\"]\n",
        "sns.boxplot(data=[topk_percentile, sigma_percentile, qf_percentile], fliersize = 0, medianprops={'color':'red', 'linewidth': 3}, color='white')\n",
        "sns.swarmplot(data=[topk_percentile, sigma_percentile, qf_percentile], color=\".25\", size = 10)\n",
        "\n",
        "ax.set_xticklabels(['TKSE', 'SoF', 'PF'])\n",
        "ax.set_ylim(top=103)\n",
        "ax.set(ylabel = 'Percentile')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gaui2bMQ4cJ9"
      },
      "source": [
        "## Max accuracies (and f1-score) comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IfGWQwJn4gJk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# diamo tutti i classificatori in input\n",
        "combs = list(combinations(clfs, len(clfs)))\n",
        "for comb in combs:\n",
        "    classifier_names = str(comb)\n",
        "        \n",
        "    accuracy_distributions = []\n",
        "    for input_clf in comb:\n",
        "        if (input_clf['type'] == 'BOS'):\n",
        "            clf = BOS_Classifier(eval(input_clf['name']))\n",
        "        elif (input_clf['type'] == 'NLP'):\n",
        "            clf = NLP_Classifier()\n",
        "        elif (input_clf['type'] == 'TSC'):\n",
        "            clf = eval(input_clf['name'])\n",
        "        accuracy_distributions.append(k_accuracy_cv(clf, X_train, y_train))\n",
        "\n",
        "    index_list, k = select_from_k_accuracy(accuracy_distributions, len(set(y_test)))\n",
        "    selected_classifiers = select_from_index(index_list, comb)\n",
        "\n",
        "    formatted_names = [re.sub(r'\\([^()]*\\)', '', x['name']) for x in comb]\n",
        "    print(\"Combination: \", formatted_names)\n",
        "    print(\"Selected classifiers: \", selected_classifiers)\n",
        "\n",
        "    # massima accuracy ottenibile con sof e i classificatori scelti\n",
        "    max_accuracy = max(results[classifier_names]['accuracies'])\n",
        "    best_ordering = results[classifier_names]['best_ordering']\n",
        "    best_sigma = results[classifier_names]['sigma']\n",
        "    ensemble = EnsembleFramework(criterion = 'sof', layers = best_ordering, params = {'sigma': best_sigma})\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    y_pred = ensemble.predict(X_test)\n",
        "    max_f1 = f1_score(y_pred, y_test, average = 'macro')*100\n",
        "    print(\"Max accuracy achievable {:.3f}\".format(max_accuracy))\n",
        "    print(\"f1 of the best combination {:.3f}\".format(max_f1))\n",
        "\n",
        "    # usiamo l'euristica sia per la selezione e l'ordinamento sia per ordinare il k\n",
        "    ensemble = EnsembleFramework(criterion = 'topk', layers = selected_classifiers, params = {'k': k})\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    y_pred = ensemble.predict(X_test)\n",
        "    k_heuristic_accuracy = ensemble.accuracy(y_pred, y_test) \n",
        "    k_heuristic_f1 = f1_score(y_pred, y_test, average = 'macro')*100\n",
        "    print(\"Accuracy with heuristic and topk {:.3f}\".format(k_heuristic_accuracy))\n",
        "    print(\"f1 with heuristic and topk {:.3f}\".format(k_heuristic_f1))\n",
        "\n",
        "    ensemble = EnsembleFramework(criterion = 'sof', layers = selected_classifiers)\n",
        "    ensemble = sigma_grid_search_cv(ensemble, X_train, y_train, sigmas)\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    y_pred = ensemble.predict(X_test)\n",
        "    sigma_heuristic_accuracy = ensemble.accuracy(y_pred, y_test)\n",
        "    sigma_heuristic_f1 = f1_score(y_pred, y_test, average = 'macro')*100\n",
        "    print(\"Accuracy with heuristic and cross-validation with sof {:.3f}\".format(sigma_heuristic_accuracy))\n",
        "    print(\"f1 with heuristic and cross-validation with sof {:.3f}\".format(sigma_heuristic_f1))\n",
        "\n",
        "    ensemble = EnsembleFramework(criterion = 'qf', layers = selected_classifiers)\n",
        "    ensemble = q_grid_search_cv(ensemble, X_train, y_train, qs)\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    y_pred = ensemble.predict(X_test)\n",
        "    qf_heuristic_accuracy = ensemble.accuracy(y_pred, y_test)\n",
        "    qf_heuristic_f1 = f1_score(y_pred, y_test, average = 'macro')*100\n",
        "\n",
        "    print(\"Accuracy with heuristic and cross-validation with qf {:.3f}\".format(qf_heuristic_accuracy))\n",
        "    print(\"f1 with heuristic and cross-validation with qf {:.3f}\".format(qf_heuristic_f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Snp7Twqr1e2U"
      },
      "outputs": [],
      "source": [
        "# diamo tutti i classificatori in input\n",
        "combs = list(combinations(clfs, 2))\n",
        "for comb in combs:\n",
        "    classifier_names = str(comb)\n",
        "        \n",
        "    accuracy_distributions = []\n",
        "    for input_clf in comb:\n",
        "        if (input_clf['type'] == 'BOS'):\n",
        "            clf = BOS_Classifier(eval(input_clf['name']))\n",
        "        elif (input_clf['type'] == 'NLP'):\n",
        "            clf = NLP_Classifier()\n",
        "        elif (input_clf['type'] == 'TSC'):\n",
        "            clf = eval(input_clf['name'])\n",
        "        accuracy_distributions.append(k_accuracy_cv(clf, X_train, y_train))\n",
        "\n",
        "    index_list, k = select_from_k_accuracy(accuracy_distributions, len(set(y_test)))\n",
        "    selected_classifiers = select_from_index(index_list, comb)\n",
        "\n",
        "    formatted_names = [re.sub(r'\\([^()]*\\)', '', x['name']) for x in comb]\n",
        "    print(\"Combination: \", formatted_names)\n",
        "    print(\"Selected classifiers: \", selected_classifiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "4QWJPxCA3jDa",
        "outputId": "4e6a1fde-5285-41b1-ac13-b1fd43101cfb"
      },
      "outputs": [],
      "source": [
        "selected_classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tu8bHxF2r90M"
      },
      "source": [
        "# k-accuracy graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "colab_type": "code",
        "id": "6r4wNoTAcZwk",
        "outputId": "637db1de-a78a-4ff7-f878-6f0b923d3133"
      },
      "outputs": [],
      "source": [
        "comb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "9yWJHDEfXxO1",
        "outputId": "0d620574-787b-41fb-dbd7-7dfad6027330"
      },
      "outputs": [],
      "source": [
        "len(accuracy_distributions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "colab_type": "code",
        "id": "z_7pjUF4CyTt",
        "outputId": "ce839fb9-fd55-4ff5-ab37-e3642e135539"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.rcParams.update({'legend.handlelength': 3.0})\n",
        "\n",
        "f, ax = plt.subplots(figsize=(7,5))\n",
        "labels = np.arange(1, 12)\n",
        "plt.plot(labels, accuracy_distributions[0], label='1NN-ED', linewidth = 5, color = 'black', linestyle = '-')\n",
        "#plt.plot(labels, accuracy_distributions[2], label='NLP', linewidth = 5, color = 'orange', linestyle = (0, (3,1,1,1)))\n",
        "plt.plot(labels, accuracy_distributions[2], label='GradBoost', linewidth = 5, color = 'grey', linestyle = ':')\n",
        "plt.plot(labels, accuracy_distributions[3], label='RF', linewidth = 5, color = 'red', linestyle = '-.')\n",
        "\n",
        "plt.plot(labels, accuracy_distributions[1], label='kNN', linewidth = 5, color = 'blue', linestyle = '--')\n",
        "plt.plot(labels, accuracy_distributions[4], label='SVM', linewidth = 5, color = 'green', linestyle = (0, (3,1,1,1,1,1)))\n",
        "#plt.grid()\n",
        "#plt.plot(labels, accuracy_distributions[4], label='SVM')\n",
        "\n",
        "plt.axis([0, 12, 72.5, 100])\n",
        "\n",
        "major_ticks = np.arange(0, 12, 2)\n",
        "minor_ticks = np.arange(0, 12, 1)\n",
        "\n",
        "ax.set_xticks(major_ticks)\n",
        "ax.set_xticks(minor_ticks, minor=True)\n",
        "\n",
        "ax.grid(which='both')\n",
        "\n",
        "#plt.title('Accuracy with varying sizes of k')\n",
        "plt.xlabel('k', fontsize = 20)\n",
        "plt.ylabel('Top-accuracy', fontsize = 20)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "#plt.savefig('thingspeak_kaccuracy.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BF-xmFNz49V5"
      },
      "source": [
        "# Comparison with Voting Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJZpThKEZuq_"
      },
      "source": [
        "Let's define a voting classifier (soft and hard voting), to see how it stacks up against our framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tW1PxZ4B5AWo"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class Voting_Classifier(BaseEstimator):\n",
        "    \n",
        "    def __init__(self, estimators, voting='hard'):\n",
        "        self.estimators = estimators\n",
        "        self.voting = voting\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        for clf in self.estimators:\n",
        "            clf.fit(X, y)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.voting == 'soft':\n",
        "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "        else:  # 'hard' voting\n",
        "            predictions = self._predict(X)\n",
        "            maj = np.apply_along_axis(\n",
        "                lambda x: np.argmax(\n",
        "                    np.bincount(x, weights=self._weights_not_none)),\n",
        "                axis=1, arr=predictions)\n",
        "\n",
        "        return maj\n",
        "    \n",
        "    def _collect_probas(self, X):\n",
        "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
        "        return np.asarray([clf.predict_proba(X) for clf in self.estimators])\n",
        "    \n",
        "    def _predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities for X in 'soft' voting \"\"\"\n",
        "        if self.voting == 'hard':\n",
        "            raise AttributeError(\"predict_proba is not available when\"\n",
        "                                 \" voting=%r\" % self.voting)\n",
        "        avg = np.average(self._collect_probas(X), axis=0)\n",
        "        return avg\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        Returns\n",
        "        -------\n",
        "        avg : array-like, shape (n_samples, n_classes)\n",
        "            Weighted average probability for each class per sample.\n",
        "        \"\"\"\n",
        "        return self._predict_proba(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H9Cz32NT5JBa"
      },
      "outputs": [],
      "source": [
        "clfs = []\n",
        "clfs.append(BOS_Classifier(eval('KNeighborsClassifier()')))\n",
        "clfs.append(BOS_Classifier(eval('GradientBoostingClassifier(random_state=100)')))\n",
        "clfs.append(BOS_Classifier(eval('RandomForestClassifier(1000, random_state=100)')))\n",
        "#clfs.append(BOS_Classifier(eval('SVC(probability=True, random_state=100)')))\n",
        "clfs.append(NLP_Classifier())\n",
        "clfs.append(TSC_1NN(metric = distance.euclidean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rRRI1hBm52sR"
      },
      "outputs": [],
      "source": [
        "voting = Voting_Classifier(clfs, voting = 'soft')\n",
        "voting.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sO1HypHt6BEc"
      },
      "outputs": [],
      "source": [
        "y_pred = voting.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "3DBSp2OB6Hpc",
        "outputId": "fae9719c-393a-41d1-9b63-cf7de0b08439"
      },
      "outputs": [],
      "source": [
        "#thingspeak\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(f1_score(y_pred, y_test, average = 'macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "0h__vu1t6ogT",
        "outputId": "9785e08f-64ba-45e9-c95c-3cc63933f94e"
      },
      "outputs": [],
      "source": [
        "#swissex\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(f1_score(y_pred, y_test, average = 'macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "lykFNd1N6qSw",
        "outputId": "c18fb11c-3006-4f4f-a040-ec01bcc01c77"
      },
      "outputs": [],
      "source": [
        "#urbanobs\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(f1_score(y_pred, y_test, average = 'macro'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "N79oufXtyDJC",
        "7C5eqOxh4JNa",
        "2RBThxWZ4JNd",
        "le2_7uKGx1Py"
      ],
      "name": "EnsembleFramework",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b9a0b1b75a443b3970ea8ab636ee145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3184223b775434cbefcb14d649e2850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ce3e4ce99b419eb541cd47f393e7b3",
            "max": 1080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b9a0b1b75a443b3970ea8ab636ee145",
            "value": 0
          }
        },
        "e5ce3e4ce99b419eb541cd47f393e7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
